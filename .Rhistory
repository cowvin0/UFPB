estacao_grupo <- dado_inteiro |>
group_by(GRUPOS) |>
distinct(BASE)
estacao_grupo |> write.csv("estacao_por_grupo.csv", row.names = FALSE)
# faz a divisão entre treinamento e teste
chuva_split <- initial_split(dado_inteiro)
chuva_train <- training(chuva_split)
chuva_test <- testing(chuva_split)
# roda o modelo linear para cada grupo
workflows <- lapply(1:5, \(x) {
workflow() |>
add_model(linear_reg()) |>
add_recipe(
dado_inteiro |>
filter(GRUPOS == x) |>
recipe(NDVI ~ ACUM90) |>
step_impute_median(c(ACUM90)) |>
step_mutate(acum2 = ACUM90**2)
)
})
# coleta as métricas do modelo
models <- lapply(1:5, \(x) {
workflows[[x]] |>
fit(dado_inteiro |> filter(GRUPOS == x)) |>
extract_fit_engine()
})
# constroí cada banco de dados com o R2 ajustado
bancos <- lapply(1:5, \(x) {
summa <- models[[x]] |> summary()
get_augment <- models[[x]] |>
augment(new_data = dado_inteiro) |>
mutate(R2 = summa$adj.r.squared,
GRUPOS = x)
if(!(".rownames" %in% colnames(get_augment))) {
get_augment
} else {
get_augment |> select(-.rownames)
}
})
# junta todos os bancos de dados
tudo_junto <- bancos |> purrr::reduce(.f = rbind)
# plota o gráfico
tudo_junto |>
mutate(ACUM90 = log1p(ACUM90)) |>
ggplot(aes(x = ACUM90, y = ..y)) +
geom_point(aes(color = R2), alpha = 0.5) +
geom_smooth(alpha = 0.5, method = "lm") +
facet_wrap(vars(GRUPOS), scales = "free") +
scale_color_viridis_c() +
labs(
y = "NDVI observado",
x = "Acumulado de 90 dias",
color = "R2"
) +
theme_bw(10, "serif")
workflows <- lapply(1:5, \(x) {
workflow() |>
add_model(linear_reg()) |>
add_recipe(
dado_inteiro |>
filter(GRUPOS == x) |>
recipe(NDVI ~ ACUM90) |>
step_impute_mean(c(ACUM90)) |>
step_mutate(acum2 = ACUM90**2)
)
})
models <- lapply(1:5, \(x) {
workflows[[x]] |>
fit(dado_inteiro |> filter(GRUPOS == x)) |>
extract_fit_engine()
})
models |> lapply(\(x) x |> summary())
library(tidymodels)
library(tidyverse)
library(knitr)
library(readxl)
library(ggplot2)
library(vip)
tidymodels_prefer()
library(GGally)
# Juntando todos os dados
set.seed(42)
arc <- "Regressao_linear(1).xlsx"
multiplesheets <- function(fname) {
sheets <- readxl::excel_sheets(fname)
tibbles <- sapply(sheets, \(x) readxl::read_excel(fname, sheet = x))
tibbles[7:47] |>
purrr::reduce(.f = rbind)
}
lat_long <- read_excel("Lat_long.xlsx")
clusters <- read_excel("Clusters.xlsx") |>
select(-Id) |>
mutate(Estacao = case_when(Estacao == "Bonito S. Fé" ~ "Bonito de Santa Fé",
Estacao == "Cach. dos Índios" ~ "Cachoeira dos Índios",
Estacao == "Aç. Eng. Ávidos" ~ "Cajazeiras/Açude Engenheiro Ávidos",
Estacao == "Aç. Lagoa do Arroz" ~ "Cajazeiras/Açude Lagoa do Arroz",
Estacao == "Coremas" ~ "Coremas/Açude Coremas",
Estacao == "Fazenda Veludo" ~ "Itaporanga/Fazenda Veludo",
Estacao == "Olho D'Água" ~ "Olho D`Água",
Estacao == "Sant. de Mangueira" ~ "Santana de Mangueira",
Estacao == "Sant. dos Garrotes" ~ "Santana dos Garrotes",
Estacao == "S.J. De Caiana" ~ "São José de Caiana",
Estacao == "São Gonçalo" ~ "Sousa/São Gonçalo",
Estacao == "S.J. de Princesa" ~ "São José de Princesa",
Estacao == "S. J.  L. Tapada" ~ "São José da Lagoa Tapada",
Estacao == "S.J. de Caiana" ~ "São José de Caiana",
Estacao == "S.J. de Piranhas" ~ "São José de Piranhas",
Estacao == "S.J. Rio do Peixe" ~ "São João do Rio do Peixe/Antenor Navarro",
TRUE ~ Estacao))
dado_inteiro <- multiplesheets(arc)
dado_inteiro <- dado_inteiro |>
mutate(BASE = ifelse(BASE %in% c("AGUIAR", "APARECIDA"), str_to_title(BASE, locale = "pt-br"), BASE))
lat_long <- lat_long |>
mutate(Cidade = ifelse(Cidade == "Olho D`Água", "Olho D'Água", Cidade),
Cidade = ifelse(Cidade == "Cajazeiras/Açude Engenheiro Avidos", "Cajazeiras/Açude Engenheiro Ávidos", Cidade)
)
dado_inteiro <- left_join(x = dado_inteiro, y = lat_long, by = join_by(BASE == Cidade)) |>
left_join(y = clusters, by = join_by(BASE == Estacao)) |>
filter(!is.na(LITO))
dado_inteiro <- dado_inteiro |>
select(-c(grupo_todos, "(x) Lat", "(y) Long", id))
dado_inteiro |> write.csv("Estacoes_juntas.csv", row.names = FALSE)
rec_int <-
recipe(NDVI ~ ALT + DECLIV + LITO, data = dado_inteiro) |>
step_string2factor(LITO) |>
step_dummy(LITO) |>
step_impute_bag(all_numeric_predictors())
# faz a clusterização utilizando apenas a altitude, itu, declividade e litologia
kclust_int <- rec_int |>
prep(strings_as_factors = FALSE) |>
bake(new_data = dado_inteiro) |>
select(-NDVI) |>
kmeans(centers = 5) |>
(\(x) { x$cluster })()
# wss <- NULL
# for (i in 1:20) {
#   kclust_int <- rec_int |>
#   prep(strings_as_factors = FALSE) |>
#   bake(new_data = dado_inteiro) |>
#   select(-c(BASE, NDVI, "DATA DA IMAGEM (DIGITAR)", ACUM30, ACUM60, ACUM90, ACUM120, ACUM180, ACUM365)) |>
#   kmeans(centers = i)
#   wss = c(wss, kclust_int$tot.withinss)
# }
# adiciona os clusters no banco de dados
dado_inteiro$GRUPOS <- kclust_int
estacao_grupo <- dado_inteiro |>
group_by(GRUPOS) |>
distinct(BASE)
estacao_grupo |> write.csv("estacao_por_grupo.csv", row.names = FALSE)
# faz a divisão entre treinamento e teste
chuva_split <- initial_split(dado_inteiro)
chuva_train <- training(chuva_split)
chuva_test <- testing(chuva_split)
# roda o modelo linear para cada grupo
workflows <- lapply(1:5, \(x) {
workflow() |>
add_model(linear_reg()) |>
add_recipe(
dado_inteiro |>
filter(GRUPOS == x) |>
recipe(NDVI ~ ACUM90) |>
step_impute_mean(c(ACUM90)) |>
step_mutate(acum2 = ACUM90**2)
)
})
# coleta as métricas do modelo
models <- lapply(1:5, \(x) {
workflows[[x]] |>
fit(dado_inteiro |> filter(GRUPOS == x)) |>
extract_fit_engine()
})
# constroí cada banco de dados com o R2 ajustado
bancos <- lapply(1:5, \(x) {
summa <- models[[x]] |> summary()
get_augment <- models[[x]] |>
augment(new_data = dado_inteiro) |>
mutate(R2 = summa$adj.r.squared,
GRUPOS = x)
if(!(".rownames" %in% colnames(get_augment))) {
get_augment
} else {
get_augment |> select(-.rownames)
}
})
# junta todos os bancos de dados
tudo_junto <- bancos |> purrr::reduce(.f = rbind)
# plota o gráfico
tudo_junto |>
mutate(ACUM90 = log1p(ACUM90)) |>
ggplot(aes(x = ACUM90, y = ..y)) +
geom_point(aes(color = R2), alpha = 0.5) +
geom_smooth(alpha = 0.5, method = "lm") +
facet_wrap(vars(GRUPOS), scales = "free") +
scale_color_viridis_c() +
labs(
y = "NDVI observado",
x = "Acumulado de 90 dias",
color = "R2"
) +
theme_bw(10, "serif")
library(tidymodels)
library(tidymodels)
library(tidyverse)
library(knitr)
library(readxl)
library(ggplot2)
library(vip)
library(ggplot2)
library(vip)
tidymodels_prefer()
library(tidymodels)
library(tidyverse)
library(knitr)
library(readxl)
library(ggplot2)
library(vip)
tidymodels_prefer()
library(GGally)
library(tidymodels)
library(tidyverse)
library(knitr)
library(readxl)
library(ggplot2)
library(vip)
tidymodels_prefer()
library(GGally)
# Juntando todos os dados
set.seed(42)
arc <- "Regressao_linear(1).xlsx"
multiplesheets <- function(fname) {
sheets <- readxl::excel_sheets(fname)
tibbles <- sapply(sheets, \(x) readxl::read_excel(fname, sheet = x))
tibbles[7:47] |>
purrr::reduce(.f = rbind)
}
lat_long <- read_excel("Lat_long.xlsx")
clusters <- read_excel("Clusters.xlsx") |>
select(-Id) |>
mutate(Estacao = case_when(Estacao == "Bonito S. Fé" ~ "Bonito de Santa Fé",
Estacao == "Cach. dos Índios" ~ "Cachoeira dos Índios",
Estacao == "Aç. Eng. Ávidos" ~ "Cajazeiras/Açude Engenheiro Ávidos",
Estacao == "Aç. Lagoa do Arroz" ~ "Cajazeiras/Açude Lagoa do Arroz",
Estacao == "Coremas" ~ "Coremas/Açude Coremas",
Estacao == "Fazenda Veludo" ~ "Itaporanga/Fazenda Veludo",
Estacao == "Olho D'Água" ~ "Olho D`Água",
Estacao == "Sant. de Mangueira" ~ "Santana de Mangueira",
Estacao == "Sant. dos Garrotes" ~ "Santana dos Garrotes",
Estacao == "S.J. De Caiana" ~ "São José de Caiana",
Estacao == "São Gonçalo" ~ "Sousa/São Gonçalo",
Estacao == "S.J. de Princesa" ~ "São José de Princesa",
Estacao == "S. J.  L. Tapada" ~ "São José da Lagoa Tapada",
Estacao == "S.J. de Caiana" ~ "São José de Caiana",
Estacao == "S.J. de Piranhas" ~ "São José de Piranhas",
Estacao == "S.J. Rio do Peixe" ~ "São João do Rio do Peixe/Antenor Navarro",
TRUE ~ Estacao))
dado_inteiro <- multiplesheets(arc)
dado_inteiro <- dado_inteiro |>
mutate(BASE = ifelse(BASE %in% c("AGUIAR", "APARECIDA"), str_to_title(BASE, locale = "pt-br"), BASE))
lat_long <- lat_long |>
mutate(Cidade = ifelse(Cidade == "Olho D`Água", "Olho D'Água", Cidade),
Cidade = ifelse(Cidade == "Cajazeiras/Açude Engenheiro Avidos", "Cajazeiras/Açude Engenheiro Ávidos", Cidade)
)
dado_inteiro <- left_join(x = dado_inteiro, y = lat_long, by = join_by(BASE == Cidade)) |>
left_join(y = clusters, by = join_by(BASE == Estacao)) |>
filter(!is.na(LITO))
dado_inteiro <- dado_inteiro |>
select(-c(grupo_todos, "(x) Lat", "(y) Long", id))
dado_inteiro |> write.csv("Estacoes_juntas.csv", row.names = FALSE)
rec_int <-
recipe(NDVI ~ ALT + DECLIV + LITO, data = dado_inteiro) |>
step_string2factor(LITO) |>
step_dummy(LITO) |>
step_impute_bag(all_numeric_predictors())
# faz a clusterização utilizando apenas a altitude, itu, declividade e litologia
kclust_int <- rec_int |>
prep(strings_as_factors = FALSE) |>
bake(new_data = dado_inteiro) |>
select(-NDVI) |>
kmeans(centers = 5) |>
(\(x) { x$cluster })()
# wss <- NULL
# for (i in 1:20) {
#   kclust_int <- rec_int |>
#   prep(strings_as_factors = FALSE) |>
#   bake(new_data = dado_inteiro) |>
#   select(-c(BASE, NDVI, "DATA DA IMAGEM (DIGITAR)", ACUM30, ACUM60, ACUM90, ACUM120, ACUM180, ACUM365)) |>
#   kmeans(centers = i)
#   wss = c(wss, kclust_int$tot.withinss)
# }
# adiciona os clusters no banco de dados
dado_inteiro$GRUPOS <- kclust_int
estacao_grupo <- dado_inteiro |>
group_by(GRUPOS) |>
distinct(BASE)
estacao_grupo |> write.csv("estacao_por_grupo.csv", row.names = FALSE)
# faz a divisão entre treinamento e teste
chuva_split <- initial_split(dado_inteiro)
chuva_train <- training(chuva_split)
chuva_test <- testing(chuva_split)
# roda o modelo linear para cada grupo
workflows <- lapply(1:5, \(x) {
workflow() |>
add_model(linear_reg()) |>
add_recipe(
dado_inteiro |>
filter(GRUPOS == x) |>
recipe(NDVI ~ ACUM90) |>
step_impute_mean(c(ACUM90)) |>
step_mutate(acum2 = ACUM90**2)
)
})
# coleta as métricas do modelo
models <- lapply(1:5, \(x) {
workflows[[x]] |>
fit(dado_inteiro |> filter(GRUPOS == x)) |>
extract_fit_engine()
})
# constroí cada banco de dados com o R2 ajustado
bancos <- lapply(1:5, \(x) {
summa <- models[[x]] |> summary()
get_augment <- models[[x]] |>
augment(new_data = dado_inteiro) |>
mutate(R2 = summa$adj.r.squared,
GRUPOS = x)
if(!(".rownames" %in% colnames(get_augment))) {
get_augment
} else {
get_augment |> select(-.rownames)
}
})
# junta todos os bancos de dados
tudo_junto <- bancos |> purrr::reduce(.f = rbind)
# plota o gráfico
tudo_junto |>
mutate(ACUM90 = log1p(ACUM90)) |>
ggplot(aes(x = ACUM90, y = ..y)) +
geom_point(aes(color = R2), alpha = 0.5) +
geom_smooth(alpha = 0.5, method = "lm") +
facet_wrap(vars(GRUPOS), scales = "free") +
scale_color_viridis_c() +
labs(
y = "NDVI observado",
x = "Acumulado de 90 dias",
color = "R2"
) +
theme_bw(10, "serif")
tudo_junto
tudo_junto |>
grupo_by(GRUPOS) |>
mutate(R2 = R2)
tudo_junto |>
group_by(GRUPOS) |>
mutate(R2 = R2)
tudo_junto |>
group_by(GRUPOS) |>
distinct(R2)
tudo_junto |>
group_by(GRUPOS) |>
distinct(R2) |>
kable()
Alguns pacotes ´uteis do R - S´eries Temporais
pacotes <- c("ts", "zoo", "xts", "forecast", "TSA", "tseries",
"gtrendsR", "ggplot2","ggthemes","patchwork","RColorBrewer",
"magrittr","dplyr", "gridExtra", "ggfortify", "fpp2", "snpar",
"dygraphs", "tseries", "forecast", "xts", "seasonalview",
"seastests", "astsa", "dygraphs", "fpp", "lmtest", "FitAR",
"deflateBR", "sidrar", "tidyverse", "ggthemes", "writexl",
"rbcb", "uroot", "rugarch", "fpp2", "dynlm", "vars", "stlplus")
pcts_instalar<-pacotes[!(pacotes %in% installed.packages()[,"Package"])]
if(length(pcts_instalar)) install.packages(pcts_instalar)
169/45
144/45
(31 - 14 - 1) ** 2 / 45
(31 - 14 - 1) ** 2
31 - 14
qchisq(0.95, 1)
144 / 31
144 / 21
pbinom(3, 10, 0.5)
pbinom(3, 10, 0.5) * 2
cos(53)
1300 - 1200 * 0.9182
sqrt(198.16)
sqrt(198.16) / 2
19 * 19
19 * 19
19 * 19 + 64
sqrt(425)
4 * 180 / 3
4 * 180 / 3 - 360
7 * 180 / 6
cos(210)
sqrt(3) / 2
cos(90)
3/ 9
4 / 9
qchisq(0.95, 1)
pbinom(1, 8, prob = 0.5)
pbinom(1, 8, prob = 0.5)
pbinom(1, 8, prob = 0.5) * 12
pbinom(1, 8, prob = 0.5) * 2
5.5 + 3 + 7.5 + 7.5 + 10.5 + 5.5 + 10.5
9 + 1.5 + 1.5 + 4
pacotes <- c("ts", "zoo", "xts", "forecast", "TSA", "tseries","gtrendsR","ggplot2",
3 "ggthemes","patchwork","RColorBrewer", "magrittr","dplyr", "gridExtra", "ggfortify",
pacotes <- c("ts", "zoo", "xts", "forecast", "TSA", "tseries","gtrendsR","ggplot2",
3 "ggthemes","patchwork","RColorBrewer", "magrittr","dplyr", "gridExtra", "ggfortify",
pacotes <- c("ts", "zoo", "xts", "forecast", "TSA", "tseries","gtrendsR","ggplot2",
"ggthemes","patchwork","RColorBrewer", "magrittr","dplyr", "gridExtra", "ggfortify",
"fpp2", "snpar", "dygraphs", "tseries", "forecast", "xts", "seasonalview", "seastests",
"astsa", "dygraphs", "fpp", "lmtest", "FitAR", "deateBR", "sidrar", "tidyverse",
"ggthemes", "writexl", "rbcb", "uroot", "rugarch", "fpp2", "dynlm", "vars", "stlplus")
pacotes_instalar <- pacotes[!(pacotes %in% installed.packages()[,"Package"])]
pacotes_instalar <- pacotes[!(pacotes %in% installed.packages()[,"Package"])]
if(length(pacotes_instalar)) install.packages(pacotes_instalar)
setwd("Documents/UFPB/")
dados1 <- readr::read_csv("heartdis.txt")
dados1 |> View()
dados1 <- readr::read_csv("heartdis.txt") |>
select(-c(caso, ))
library(tidyverse)
dados1 <- readr::read_csv("heartdis.txt") |>
select(-caso)
library(tidyverse)
dados1 <- readr::read_csv("heartdis.txt") |>
select(-caso)
dados1 |> View()
dados1
modelo1 <- glm(HeartDisease ~ x1 + x2 + x3  + x4 + x5 data = dados1)
modelo1 <- glm(HeartDisease ~ x1 + x2 + x3  + x4 + x5, family = binomial(link = "logit") data = dados1)
modelo1 <- glm(HeartDisease ~ x1 + x2 + x3  + x4 + x5, family = binomial(link = "logit"), data = dados1)
modelo1 <- glm(HeartDisease ~ x1 + x2 + x3  + x4 + x5,
family = binomial(link = "logit"),
data = dados1)
modelo1
dados1 |> summary()
modelo1 |> summary()
modelo1
summary(modelo1)
summary(modelo1)$dispersion
summary(modelo1)$deviance
summary(modelo1)
desvio1 <- summary(modelo1)$deviance
phi1 <- summary(modelo1)$dispersion
phi1
summ <- summary(modelo1)
summ
summ$dispersion
summ
q.quadr1 <- qchisq(0.95, desvio1)
phi1 <- summary(modelo1)$dispersion
desvio1 <- summary(modelo1)$deviance / phi1
q.quadr1 <- qchisq(0.95, desvio1)
q.quadr1
q.quadr1
desvio1
q.quadr1
summary(modelo1)
install.packages("Epi")
install.packages("ROCR")
install.packages("caret")
install.packages("pROC")
library(pROC)
library(tidyverse)
library(caret)
library(pROC)
dados1 <- readr::read_csv("heartdis.txt") |>
select(-caso)
modelo1 <- glm(HeartDisease ~ x1 + x2 + x3  + x4 + x5,
family = binomial(link = "logit"),
data = dados1)
phi1 <- summary(modelo1)$dispersion
desvio1 <- summary(modelo1)$deviance / phi1
q.quadr1 <- qchisq(0.95, desvio1)
predict(modelo1, dados1)
dados1$HeartDisease
probs_previstas <- predict(modelo1, dados1, type = "response")
probs_previstas
probs_previstas <- predict(modelo1, dados1, type = "response")
classes_previstas <- ifelse(probs_previstas > 0.5, 1, 0)
classes_previstas
cm <- confusionMatrix(table(predicted_classes, titanic_test$Survived))
cm <- confusionMatrix(table(classes_previstas, dados1$HeartDisease))
cm
library(tidyverse)
library(caret)
library(pROC)
dados1 <- readr::read_csv("heartdis.txt") |>
select(-caso)
modelo1 <- glm(HeartDisease ~ x1 + x2 + x3  + x4 + x5,
family = binomial(link = "logit"),
data = dados1)
phi1 <- summary(modelo1)$dispersion
desvio1 <- summary(modelo1)$deviance / phi1
q.quadr1 <- qchisq(0.95, desvio1)
probs_previstas <- predict(modelo1, dados1, type = "response")
classes_previstas <- ifelse(probs_previstas > 0.5, 1, 0)
# matriz de confusão
cm <- confusionMatrix(table(classes_previstas, dados1$HeartDisease))
cm
dados1$HeartDisease
dados1$HeartDisease |> table()
302 / 160
roc_obj <- roc(dados1$HeartDisease, probs_previstas)
roc_obj
roc_obj <- roc(dados1$HeartDisease, probs_previstas)
plot(roc_obj, main = "Curva ROC para o modelo de regressão logística")
abline(0, 1, lty = 2, col = "black")
roc_obj <- roc(dados1$HeartDisease, probs_previstas)
plot(roc_obj, main = "Curva ROC para o modelo de regressão logística")
abline(0, 1, lty = 2, col = "black")
auc(roc_obj)
